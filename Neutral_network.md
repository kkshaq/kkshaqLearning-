# Neutral network 
神经网络起源于多层感知机(Multi Layer Perceptron)，用于解决Single Perceptron函数拟合能力的欠缺，单层感知机是二分类的线性分类模型，单层感知机无法解决异或问题，只能拟合线性函数，如果要拟合非线性函数，需要多层感知机，多层感知机可以区分异或问题。
对于二分类的线性可分问题，阶跃函数常常作为激励函数activation function；MLP处理非线性可分问题，常用sigmoid function,Relu作为激励函数。由于历史问题，多层神经网络也叫做MLP, 数学上可以证明，单个隐藏层的MLP就可以无限逼近拟合函数。

## Netral network structure
$ a_1^{(2)} &= f(W_{11}^{(1)}x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)}) \\ $
a_2^{(2)} &= f(W_{21}^{(1)}x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 + b_2^{(1)})  \\
a_3^{(2)} &= f(W_{31}^{(1)}x_1 + W_{32}^{(1)} x_2 + W_{33}^{(1)} x_3 + b_3^{(1)})  \\
h_{W,b}(x) &= a_1^{(3)} =  f(W_{11}^{(2)}a_1^{(2)} + W_{12}^{(2)} a_2^{(2)} + W_{13}^{(2)} a_3^{(2)} + b_1^{(2)}) 


